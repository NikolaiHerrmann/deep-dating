{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_dating.networks import DatingCNN\n",
    "from deep_dating.datasets import DatingDataLoader, DatasetName, SetType\n",
    "from deep_dating.metrics import DatingMetrics\n",
    "from deep_dating.preprocessing import PreprocessRunner\n",
    "from deep_dating.util import SEED, save_figure\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_patches(all_labels, all_outputs, all_paths, p=None):\n",
    "\n",
    "    preds = {}\n",
    "\n",
    "    for i, img_name in enumerate(all_paths):\n",
    "        img_name = PreprocessRunner.get_base_img_name(img_name)\n",
    "        if p and img_name not in p:\n",
    "            continue\n",
    "        if not img_name in preds:\n",
    "            preds[img_name] = [all_labels[i], [all_outputs[i]]]\n",
    "        else:\n",
    "            preds[img_name][1].append(all_outputs[i])\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    if p is None:\n",
    "        p = preds\n",
    "\n",
    "    for key, val in p.items():\n",
    "        preds[key][1] = np.mean(preds[key][1], axis=0)\n",
    "        features.append(preds[key][1])\n",
    "        labels.append(preds[key][0])\n",
    "        # print(preds[key][1].shape)\n",
    "        # exit()\n",
    "    features = np.array(features)\n",
    "\n",
    "    return np.array(labels), features, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"runs/Feb15-19-21-40/model_epoch_0_feats_train.pkl\", \"rb\") as f:\n",
    "    labels_train_low, features_train_low, all_paths_train_low = pickle.load(f)\n",
    "    labels_train_low = labels_train_low.flatten()\n",
    "    labels_train_low, features_train_low, train_p = merge_patches(labels_train_low, features_train_low, all_paths_train_low)\n",
    "\n",
    "with open(\"runs/Feb15-19-21-40/model_epoch_0_feats_val.pkl\", \"rb\") as f:\n",
    "    labels_val_low, features_val_low, all_paths_val_low = pickle.load(f)\n",
    "    labels_val_low = labels_val_low.flatten()\n",
    "    labels_val_low, features_val_low, val_p = merge_patches(labels_val_low, features_val_low, all_paths_val_low)\n",
    "\n",
    "with open(\"runs/Feb18-10-52-17/model_epoch_1_feats_train.pkl\", \"rb\") as f:\n",
    "    labels_train_high, features_train_high, all_paths_train_high = pickle.load(f)\n",
    "    labels_train_high = labels_train_high.flatten()\n",
    "    labels_train_high, features_train_high, _ = merge_patches(labels_train_high, features_train_high, all_paths_train_high, train_p)\n",
    "\n",
    "with open(\"runs/Feb18-10-52-17/model_epoch_1_feats_val.pkl\", \"rb\") as f:\n",
    "    labels_val_high, features_val_high, all_paths_val_high = pickle.load(f)\n",
    "    labels_val_high = labels_val_high.flatten()\n",
    "    labels_val_high, features_val_high, _ = merge_patches(labels_val_high, features_val_high, all_paths_val_high, val_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_high.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_low.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 15:40:17.809187: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-27 15:40:17.809225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-27 15:40:17.827550: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-27 15:40:17.857629: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-27 15:40:19.113523: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_x = np.concatenate([features_train_low, features_val_low])\n",
    "new_train_x = scaler.fit_transform(new_train_x)\n",
    "new_val_x = np.hstack([features_val_low])\n",
    "new_val_x = scaler.transform(new_val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC() #RandomForestClassifier(random_state=43) #MLPClassifier(batch_size=32, solver=\"adam\", hidden_layer_sizes=(1536, 1000, 512), verbose=True, early_stopping=True, n_iter_no_change=5)\n",
    "\n",
    "svm.fit(features_train_low, labels_train_low)\n",
    "labels_val_low_predict = svm.predict(features_val_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(labels_train_low)\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = label_encoder.transform(labels_val_low)\n",
    "y_val = to_categorical(y_val)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(1536)),\n",
    "        # layers.Dense(2048, activation=\"relu\"),\n",
    "        # #layers.Dropout(0.3),\n",
    "        # #layers.BatchNormalization(),\n",
    "\n",
    "\n",
    "        layers.Dense(1024, activation=\"relu\"),\n",
    "        #layers.Dropout(0.3),\n",
    "        #layers.BatchNormalization(),\n",
    "\n",
    "\n",
    "        layers.Dense(1024, activation=\"relu\"),\n",
    "        #layers.Dropout(0.3),\n",
    "        #layers.BatchNormalization(),\n",
    "\n",
    "\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        # layers.Dropout(0.3),\n",
    "        # layers.BatchNormalization(),\n",
    "\n",
    "\n",
    "        layers.Dense(15, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.00001), metrics=[\"accuracy\"])\n",
    "model.fit(new_train_x, y_train, batch_size=batch_size, epochs=epochs, validation_data=(new_val_x, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.00423728813559 1980.247175141243\n",
      "[0, 25]\n",
      "[64.97175141242938, 80.64971751412429, 91.66666666666666]\n"
     ]
    }
   ],
   "source": [
    "alphas = [0, 25, 50]\n",
    "metrics = DatingMetrics(alphas=alphas)\n",
    "metrics.names\n",
    "\n",
    "vals = metrics.calc(labels_val_low, labels_val_low_predict)\n",
    "\n",
    "mae, mse = tuple(vals[:2])\n",
    "cs_ = vals[2:]\n",
    "\n",
    "print(mae, mse)\n",
    "print([0, 25])\n",
    "print(cs_)\n",
    "\n",
    "# 19.00423728813559 1980.247175141243\n",
    "# [0, 25]\n",
    "# [64.97175141242938, 80.64971751412429, 91.66666666666666]\n",
    "\n",
    "# 19.326271186440678 2154.9138418079096\n",
    "# [0, 25]\n",
    "# [65.11299435028248, 80.9322033898305, 91.38418079096046]\n",
    "\n",
    "# 32.978813559322035 5692.001412429378\n",
    "# [0, 25]\n",
    "# [58.47457627118644, 72.03389830508475, 83.75706214689266]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
